{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_dir = \"C:\\\\Users\\\\talha\\\\Google Drive\\\\virginia tech\\\\Computer Science\\\\Deep Learning\\\\assignments\\\\homework2\\\\sentiment-data\"\n",
    "_dir = \"/home/talha/deeplearning/sentiment-data/\"\n",
    "\n",
    "def load_data(_dir):\n",
    "    train_file = _dir + \"train.csv\"\n",
    "    test_file = _dir + \"test.csv\"\n",
    "    word_vec_file = _dir + \"word-vectors.txt\"\n",
    "\n",
    "    train_data = np.genfromtxt(train_file, dtype=str, delimiter=',')\n",
    "    test_data = np.genfromtxt(test_file, dtype=str, delimiter=',')\n",
    "    word_vec = np.genfromtxt(word_vec_file,dtype=str,delimiter=',')\n",
    "    return (train_data, test_data, word_vec)\n",
    "\n",
    "def get_data(_dir):\n",
    "    # Load the data\n",
    "    (train_data, test_data, word_vec) = load_data(_dir)\n",
    "    \n",
    "    num_word_vec_encodings = word_vec.shape[0]\n",
    "    word_vec_dim = word_vec.shape[1] - 1\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = word_vec.reshape(word_vec.shape[0], word_vec_dim + 1)\n",
    "    word_vec_df = pd.DataFrame(index=word_vec[:, 0], data=list(word_vec[:,np.arange(1,word_vec_dim+1)]))\n",
    "    \n",
    "    assert word_vec_df.shape == (num_word_vec_encodings, word_vec_dim)\n",
    "    \n",
    "    train_x = train_data[:,1]\n",
    "    train_y = train_data[:,0]\n",
    "\n",
    "    test_x = test_data[:,1]\n",
    "    test_y = test_data[:,0]\n",
    "\n",
    "    # encode positive class as +1 and negative class as -1\n",
    "    train_y[train_y=='postive']=1\n",
    "    train_y[train_y=='negative']=0\n",
    "    test_y[test_y=='postive']=1\n",
    "    test_y[test_y=='negative']=0\n",
    "\n",
    "    # convert datatype of class encodings\n",
    "    train_y=train_y.astype(int)\n",
    "    test_y=test_y.astype(int)\n",
    "    \n",
    "    return (train_x, train_y, test_x, test_y, word_vec, word_vec_df)\n",
    "\n",
    "(train_x, train_y, test_x, test_y, word_vec, word_vec_df) = get_data(_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(20000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(317934, 50)\n"
     ]
    }
   ],
   "source": [
    "print (train_x.shape)\n",
    "print (train_y.shape)\n",
    "print (test_x.shape)\n",
    "print (test_y.shape)\n",
    "\n",
    "# assert that data loaded correctly\n",
    "assert train_x.shape == (20000,)\n",
    "assert train_y.shape == (20000,)\n",
    "assert test_x.shape == (5000,)\n",
    "assert test_y.shape == (5000,)\n",
    "\n",
    "# map each word to an index\n",
    "# at each of those idices, the mapping is placed in word_vec\n",
    "word_indices_list = list(word_vec[:,0])\n",
    "#print(word_indices_list[-10:-1])\n",
    "#print(word_indices_list.index('zsombor'))\n",
    "\n",
    "# word_vectors: Contains vector representations of the words\n",
    "word_vectors = word_vec[:,1:51]\n",
    "#word_vectors = word_vectors.astype(dtype=np.float)\n",
    "print(word_vectors.shape)\n",
    "word_vectors = word_vectors.astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317935, 50)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros((50))\n",
    "word_vectors = np.vstack((word_vectors,zeros))\n",
    "print(word_vectors.shape)\n",
    "print(word_vectors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSentences = train_x.shape[0]\n",
    "maxSeqLength = 100\n",
    "\n",
    "ids = np.full((trainSentences, maxSeqLength), word_vectors.shape[0]-1, dtype='int32')\n",
    "sentence_counter=0\n",
    "for sentence in train_x:\n",
    "    split = sentence.split()\n",
    "    word_counter=0\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[sentence_counter][word_counter] = word_indices_list.index(word)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        word_counter=word_counter+1    \n",
    "        if word_counter >= maxSeqLength:\n",
    "            break\n",
    "    sentence_counter = sentence_counter+1\n",
    "np.save('train_matrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For test data\n",
    "testSentences = test_x.shape[0]\n",
    "maxSeqLength = 100\n",
    "\n",
    "test_ids = np.full((testSentences, maxSeqLength), word_vectors.shape[0]-1, dtype='int32')\n",
    "sentence_counter=0\n",
    "for sentence in test_x:\n",
    "    split = sentence.split()\n",
    "    word_counter=0\n",
    "    for word in split:\n",
    "        try:\n",
    "            test_ids[sentence_counter][word_counter] = word_indices_list.index(word)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        word_counter=word_counter+1    \n",
    "        if word_counter >= maxSeqLength:\n",
    "            break\n",
    "    sentence_counter = sentence_counter+1\n",
    "np.save('test_matrix', test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also save these, needed in training\n",
    "np.save('word_vectors', word_vectors)\n",
    "np.save('sen_train_y', train_y)\n",
    "np.save('sen_test_y', test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data that's needed for training.\n",
    "# Can do this step instead of loading all the data and performing word-to-vector \n",
    "# transformations.\n",
    "ids = np.load('train_matrix.npy')\n",
    "test_ids = np.load('test_matrix.npy')\n",
    "word_vectors = np.load('word_vectors.npy')\n",
    "train_y = np.load('sen_train_y.npy')\n",
    "test_y = np.load('sen_test_y.npy')\n",
    "word_vectors=word_vectors.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n",
      "(5000, 100)\n",
      "(20000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "print(ids.shape)\n",
    "print(test_ids.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_dir = \"/home/talha/deeplearning/sentiment-data/\"\n",
    "#word_vec_file=_dir+\"word-vectors.txt\"\n",
    "#word_vec = np.genfromtxt(word_vec_file,dtype=str,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and testing encoding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.43943     0.29657999  0.44867    ..., -1.06570005  0.43312001\n",
      "    0.24698   ]\n",
      "  [ 0.11626     0.53896999 -0.39513999 ..., -0.39061999 -0.10885     0.084513  ]\n",
      "  [-0.75335997  0.54070002  0.064126   ..., -1.0632     -0.76071\n",
      "    1.03789997]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "_ids_test = ids[2].reshape(1,100)\n",
    "with tf.device('/cpu:0'):\n",
    "    input_data = tf.placeholder(tf.int32, [1, 100], name='test_placeholder')\n",
    "    encoded_data = tf.Variable(tf.zeros([1, 100, 50]), dtype=tf.float32)\n",
    "    encoded_data = tf.nn.embedding_lookup(word_vectors, input_data)\n",
    "\n",
    "sess = tf.Session()\n",
    "x = sess.run(encoded_data, feed_dict={input_data: _ids_test})\n",
    "print(x)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN on sentiment analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, cost = 0.00171604360035\n",
      "Epoch: 1, Accuracy = 0.9992\n",
      "Epoch: 2, cost = 7.02255456963e-07\n",
      "Epoch: 2, Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "max_sequence_length = 100\n",
    "vector_dimensions = 50\n",
    "batch_size = 16\n",
    "rnn_units = 100\n",
    "num_classes = 2\n",
    "train_examples = train_y.shape[0]\n",
    "train_iters = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Placeholders for model input\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, max_sequence_length])\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "    # Variable for getting encoded data for input vectors\n",
    "    encoded_data = tf.Variable(tf.zeros([batch_size, max_sequence_length, vector_dimensions]), dtype=tf.float32)\n",
    "    encoded_data = tf.nn.embedding_lookup(word_vectors, input_data)\n",
    "\n",
    "    # Hidden Layer of RNN cells\n",
    "    rnn_cell = tf.contrib.rnn.BasicRNNCell(rnn_units, activation=tf.tanh)    \n",
    "    \n",
    "    #The model converges quickly in 2 epochs and gives ~70% accuracy on test data.\n",
    "    #Adding dropout (without dropout, converged quickly in 2 epochs but gave 70%\n",
    "    #accuracy on test data. Similar 70.83% accuracy was achieved with 5 epochs).\n",
    "    #Might be because of overfitting, so dropout seems necessary here\n",
    "    #With dropout and 5 training iterations, getting 56.27 % accuracy on test data\n",
    "    #Should try either of two things now.. 1) decrease keep_prob from 0.75 2) train for less number of epochs. Trying 2 first\n",
    "    #Training for less number of epochs has not helped.\n",
    "    #So now trying with varying the batch sizes of the input data.\n",
    "    #Larger batch sizes train quickly but giving me low accuracy (~50 to 59%)\n",
    "    #Tried with smaller batch sizes: batch_size = 10, gives me accuracy of 65%\n",
    "    #But for this, the accuracy on training data is 93% only and the cost/loss \n",
    "    #I get is 0.41. So, it seems I can try to multiple iterations.\n",
    "    \n",
    "    #rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, output_keep_prob=0.5)\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, encoded_data, dtype=tf.float32)\n",
    "\n",
    "    # Weights for output layers\n",
    "    W_o = tf.Variable(tf.truncated_normal([rnn_units, num_classes]), name='W_o')\n",
    "    b_o = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b_o')\n",
    "\n",
    "\n",
    "    #get the output of the final cell\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    final_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1) \n",
    "\n",
    "    # Logits\n",
    "    logits = (tf.matmul(final_output, W_o) + b_o)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    rnn_sess = tf.Session()\n",
    "    rnn_sess.run(init)\n",
    "\n",
    "for epoch in range(train_iters):\n",
    "    avg_cost = 0.\n",
    "    avg_acc = 0.\n",
    "    total_batch = int(train_examples/batch_size)\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        offset = (i * batch_size) % (batch_size)\n",
    "        batch_x = ids[offset:offset+batch_size]\n",
    "        batch_y = tf.one_hot(train_y[offset:offset+batch_size], depth=num_classes).eval(session=rnn_sess)\n",
    "\n",
    "        # Run training step and cost op \n",
    "        t, c, a = rnn_sess.run(\n",
    "                        [optimizer, loss, accuracy], \n",
    "                        feed_dict={input_data: batch_x, labels: batch_y}\n",
    "                        )\n",
    "        avg_cost += c\n",
    "        avg_acc += a\n",
    "\n",
    "    # Compute average cost\n",
    "    avg_cost /= total_batch\n",
    "    avg_acc /= total_batch\n",
    "\n",
    "    print(\"Epoch: \" + str(epoch+1) + \", cost = \" + str(avg_cost)) \n",
    "    print(\"Epoch: \" + str(epoch+1) + \", Accuracy = \" + str(avg_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 68.75 %\n"
     ]
    }
   ],
   "source": [
    "test_iters = math.floor(test_y.shape[0] / batch_size)\n",
    "\n",
    "test_accuracy = 0\n",
    "for i in range(test_iters):\n",
    "    offset = (i * batch_size) % (batch_size)\n",
    "    batch_x = test_ids[offset:offset+batch_size]\n",
    "    batch_y = tf.one_hot(train_y[offset:offset+batch_size], depth=num_classes).eval(session=rnn_sess)\n",
    "    \n",
    "    test_accuracy += (rnn_sess.run(accuracy, feed_dict={input_data: batch_x, labels: batch_y}))\n",
    "\n",
    "test_accuracy /= test_iters\n",
    "print(\"Accuracy on test data = \" + str(test_accuracy * 100) + \" %\")\n",
    "\n",
    "rnn_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
